name: Automated Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'requirements*.txt'
      - 'pyproject.toml'
      - '.github/workflows/tests.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'requirements*.txt'
      - 'pyproject.toml'
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'unit'
          - 'integration'
          - 'performance'
          - 'security'
          - 'compatibility'

env:
  PYTHON_DEFAULT: '3.11'
  FORCE_COLOR: 1
  PYTEST_ADDOPTS: '--color=yes'

jobs:
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-22.04
    outputs:
      src-changed: ${{ steps.changes.outputs.src }}
      tests-changed: ${{ steps.changes.outputs.tests }}
      deps-changed: ${{ steps.changes.outputs.deps }}
      skip-tests: ${{ steps.skip.outputs.skip }}
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Detect file changes
      uses: dorny/paths-filter@v2
      id: changes
      with:
        filters: |
          src:
            - 'src/**'
          tests:
            - 'tests/**'
          deps:
            - 'requirements*.txt'
            - 'pyproject.toml'
            - 'setup.py'
    
    - name: Check if we should skip tests
      id: skip
      run: |
        if [[ "${{ github.event_name }}" == "push" ]]; then
          # Check commit message for [skip tests]
          if git log -1 --pretty=%B | grep -q "\[skip tests\]"; then
            echo "skip=true" >> $GITHUB_OUTPUT
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "skip=false" >> $GITHUB_OUTPUT
        fi

  unit-tests:
    name: Unit Tests
    needs: detect-changes
    if: needs.detect-changes.outputs.skip-tests != 'true'
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, windows-2022, macos-12]
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']
        exclude:
          # Reduce matrix on scheduled runs
          - os: windows-2022
            python-version: '3.8'
          - os: windows-2022  
            python-version: '3.9'
          - os: macos-12
            python-version: '3.8'
          - os: macos-12
            python-version: '3.9'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        cache-dependency-path: 'requirements*.txt'
    
    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-22.04'
      run: |
        sudo apt-get update
        sudo apt-get install -y tesseract-ocr tesseract-ocr-tur ghostscript poppler-utils qpdf
    
    - name: Install system dependencies (macOS)
      if: matrix.os == 'macos-12'
      run: |
        brew install tesseract tesseract-lang ghostscript poppler qpdf
    
    - name: Install system dependencies (Windows)
      if: matrix.os == 'windows-2022'
      run: |
        choco install tesseract --pre
        choco install ghostscript
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test]
    
    - name: Create test environment
      run: |
        mkdir -p tests/temp tests/output
        python -c "
        import pypdf_tools
        print('PyPDF-Tools version:', pypdf_tools.__version__)
        print('Python version:', __import__('sys').version)
        "
    
    - name: Run unit tests with coverage
      run: |
        pytest tests/unit/ -v \
          --cov=pypdf_tools \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --cov-branch \
          --cov-fail-under=75 \
          --junitxml=test-results-unit.xml \
          --tb=short
    
    - name: Upload coverage to Codecov
      if: matrix.os == 'ubuntu-22.04' && matrix.python-version == env.PYTHON_DEFAULT
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unit,python${{ matrix.python-version }},${{ matrix.os }}
        name: Unit Tests Coverage
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: unit-test-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: |
          test-results-unit.xml
          htmlcov/
        retention-days: 7

  integration-tests:
    name: Integration Tests
    needs: detect-changes
    if: needs.detect-changes.outputs.skip-tests != 'true'
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, windows-2022, macos-12]
        test-group: [pdf-operations, ocr-operations, security-operations, automation]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        if [[ "${{ matrix.os }}" == "ubuntu-22.04" ]]; then
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr tesseract-ocr-all libreoffice ghostscript poppler-utils qpdf
        elif [[ "${{ matrix.os }}" == "macos-12" ]]; then
          brew install tesseract tesseract-lang libreoffice ghostscript poppler qpdf
        elif [[ "${{ matrix.os }}" == "windows-2022" ]]; then
          choco install tesseract --pre
          choco install libreoffice
          choco install ghostscript
        fi
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,integration]
    
    - name: Download test datasets
      run: |
        python scripts/download_test_data.py --integration
    
    - name: Run integration tests
      timeout-minutes: 30
      run: |
        pytest tests/integration/ -v \
          -k "${{ matrix.test-group }}" \
          --junitxml=test-results-integration-${{ matrix.test-group }}.xml \
          --tb=short \
          --durations=10
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: integration-test-results-${{ matrix.os }}-${{ matrix.test-group }}
        path: |
          test-results-integration-*.xml
          tests/temp/
          tests/output/
        retention-days: 7

  performance-tests:
    name: Performance Tests
    needs: detect-changes
    if: needs.detect-changes.outputs.skip-tests != 'true' && (github.event_name == 'schedule' || contains(github.event.inputs.test_suite, 'performance') || contains(github.event.inputs.test_suite, 'all'))
    runs-on: ubuntu-22.04
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y tesseract-ocr tesseract-ocr-all libreoffice
        python -m pip install --upgrade pip
        pip install -e .[test,benchmark]
    
    - name: Download performance datasets
      run: |
        python scripts/download_test_data.py --performance
    
    - name: Run performance benchmarks
      run: |
        pytest tests/performance/ -v \
          --benchmark-only \
          --benchmark-warmup=off \
          --benchmark-disable-gc \
          --benchmark-json=benchmark-results.json \
          --benchmark-save=github-${{ github.run_number }} \
          --benchmark-save-data
    
    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      if: github.ref == 'refs/heads/main'
      with:
        tool: 'pytest'
        output-file-path: benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '150%'
        fail-on-alert: false
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: |
          benchmark-results.json
          .benchmarks/
        retention-days: 30

  security-tests:
    name: Security Tests
    needs: detect-changes
    if: needs.detect-changes.outputs.skip-tests != 'true'
    runs-on: ubuntu-22.04
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,security]
    
    - name: Run security tests
      run: |
        pytest tests/security/ -v \
          --junitxml=test-results-security.xml \
          --tb=short
    
    - name: Run bandit security scan
      run: |
        bandit -r src/ -f json -o bandit-results.json || true
        bandit -r src/ -f txt -o bandit-results.txt || true
    
    - name: Run safety check
      run: |
        safety check --json --output safety-results.json || true
        safety check --output safety-results.txt || true
    
    - name: Upload security results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: security-results
        path: |
          test-results-security.xml
          bandit-results.*
          safety-results.*
        retention-days: 30

  compatibility-tests:
    name: Compatibility Tests
    needs: detect-changes
    if: needs.detect-changes.outputs.skip-tests != 'true' && (github.event_name == 'schedule' || contains(github.event.inputs.test_suite, 'compatibility') || contains(github.event.inputs.test_suite, 'all'))
    runs-on: ubuntu-22.04
    strategy:
      matrix:
        test-category:
          - pdf-versions  # PDF 1.3, 1.4, 1.5, 1.6, 1.7, 2.0
          - large-files   # Files > 100MB
          - unicode-content  # Unicode filenames and content
          - legacy-formats   # Old PDF formats
          - corrupted-files  # Partially corrupted PDFs
          - encrypted-pdfs   # Various encryption methods
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y tesseract-ocr tesseract-ocr-all
        python -m pip install --upgrade pip
        pip install -e .[test]
    
    - name: Download compatibility test data
      run: |
        python scripts/download_test_data.py --compatibility --category ${{ matrix.test-category }}
    
    - name: Run compatibility tests
      timeout-minutes: 45
      run: |
        pytest tests/compatibility/ -v \
          -k "${{ matrix.test-category }}" \
          --junitxml=test-results-compat-${{ matrix.test-category }}.xml \
          --tb=short \
          --maxfail=10
    
    - name: Upload compatibility results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: compatibility-results-${{ matrix.test-category }}
        path: |
          test-results-compat-*.xml
          tests/compatibility/reports/
        retention-days: 7

  ui-tests:
    name: UI Tests
    needs: detect-changes
    if: needs.detect-changes.outputs.skip-tests != 'true' && (github.event_name == 'schedule' || contains(github.event.inputs.test_suite, 'all'))
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-22.04, windows-2022, macos-12]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}
        cache: 'pip'
    
    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-22.04'
      run: |
        sudo apt-get update
        sudo apt-get install -y xvfb libqt6widgets6 tesseract-ocr
        # Setup virtual display
        export DISPLAY=:99
        Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
    
    - name: Install system dependencies (macOS)
      if: matrix.os == 'macos-12'
      run: |
        brew install tesseract
    
    - name: Install system dependencies (Windows)
      if: matrix.os == 'windows-2022'
      run: |
        choco install tesseract --pre
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,ui]
    
    - name: Run UI tests
      env:
        DISPLAY: ':99'
      run: |
        pytest tests/ui/ -v \
          --junitxml=test-results-ui.xml \
          --tb=short \
          --durations=5
    
    - name: Upload UI test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: ui-test-results-${{ matrix.os }}
        path: |
          test-results-ui.xml
          tests/ui/screenshots/
        retention-days: 7

  api-tests:
    name: API Tests
    needs: detect-changes
    if: needs.detect-changes.outputs.skip-tests != 'true'
    runs-on: ubuntu-22.04
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y tesseract-ocr
        python -m pip install --upgrade pip
        pip install -e .[test,api]
    
    - name: Start API server
      run: |
        python -m pypdf_tools.api --port 8080 --host localhost &
        API_PID=$!
        echo "API_PID=$API_PID" >> $GITHUB_ENV
        
        # Wait for server to start
        sleep 5
        curl -f http://localhost:8080/health || exit 1
    
    - name: Run API tests
      run: |
        pytest tests/api/ -v \
          --junitxml=test-results-api.xml \
          --tb=short
    
    - name: Stop API server
      if: always()
      run: |
        if [[ -n "$API_PID" ]]; then
          kill $API_PID || true
        fi
    
    - name: Upload API test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: api-test-results
        path: test-results-api.xml
        retention-days: 7

  test-summary:
    name: Test Summary
    needs: [unit-tests, integration-tests, performance-tests, security-tests, compatibility-tests, ui-tests, api-tests]
    if: always() && needs.detect-changes.outputs.skip-tests != 'true'
    runs-on: ubuntu-22.04
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3
      with:
        path: test-results/
    
    - name: Generate test report
      run: |
        python -c "
        import os
        import xml.etree.ElementTree as ET
        from pathlib import Path
        
        total_tests = 0
        total_failures = 0
        total_errors = 0
        total_skipped = 0
        
        for xml_file in Path('test-results').rglob('*.xml'):
            try:
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                if root.tag == 'testsuite':
                    total_tests += int(root.get('tests', 0))
                    total_failures += int(root.get('failures', 0))
                    total_errors += int(root.get('errors', 0))
                    total_skipped += int(root.get('skipped', 0))
                elif root.tag == 'testsuites':
                    for suite in root.findall('testsuite'):
                        total_tests += int(suite.get('tests', 0))
                        total_failures += int(suite.get('failures', 0))
                        total_errors += int(suite.get('errors', 0))
                        total_skipped += int(suite.get('skipped', 0))
            except Exception as e:
                print(f'Error parsing {xml_file}: {e}')
        
        print(f'# Test Summary')
        print(f'')
        print(f'- **Total Tests**: {total_tests}')
        print(f'- **Passed**: {total_tests - total_failures - total_errors - total_skipped}')
        print(f'- **Failed**: {total_failures}')
        print(f'- **Errors**: {total_errors}')
        print(f'- **Skippe
